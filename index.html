<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shehreen Azad</title>

    <meta name="author" content="Shehreen Azad">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shehreen Azad
                </p>
                <p>
                  I am a fourth year PhD student at <a href="https://www.crcv.ucf.edu">Center for Research in Computer Vision (CRCV)</a>, University of Central Florida (UCF), under the supervision of <a href="https://www.crcv.ucf.edu/person/rawat/" style="font-size:15px">Dr Yogesh Singh Rawat</a>. </p>

                  <p style="font-size:15px">
                  I have a broad interest in deep learning and computer vision. My research mainly focuses on <strong>long-form video</strong> understanding and <strong>multimodal reasoning.</strong></p>
                 
                  <p style="color:red;"><b> Currently looking for internship positions for Summer'26! Feel free to drop me an email. </b></p>
                
                </p>
                <p style="text-align:center">
                  <a href="mailto:shehreen.azad@ucf.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=2SqIiLwAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="data/Resume_ShehreenAzad.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/shehreen-azad-89b1bb26a/">LinkedIn</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile_img.jpg"><img style="width:100%;max-width:100%;" alt="profile photo" src="images/profile_img.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">


		        <h2>Updates</h2>
                  <p>
                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jun'25:</span> <a href="#disenq" style="text-decoration: none;"> <strong>One first-author paper</strong></a> got accepted in ICCV 2025 as Highlight. ‚≠êÔ∏èüî• <br>

                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'25:</span> <a href="#geometer" style="text-decoration: none; ">
                    <strong> One benchmark paper</strong> </a> got accepted in CVPR MMFM Workshop 2025. <br>

                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Feb'25:</span> <a href="#hierarq" style="text-decoration: none;">
                      <strong>One first-author paper</strong></a> got accepted in CVPR'25. üî• <br>

                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'24:</span> <a href="#robustness" style="text-decoration: none; "> <strong> Two benchmark papers </strong></a> got accepted in CVPR MMFM Workshop 2024. <br>

                    <!-- <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'24:</span> <a href="#compositionality" style="text-decoration: none; "><strong> Benchmark paper </strong></a> got accepted in CVPR MMFM Workshop 2024. <br> -->
                    
                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Feb'24:</span> <a href="#abnet" style="text-decoration: none;"><strong>One first-author paper</strong></a> got accepted in CVPR'24. üî• <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            
            <p>Below is a selected list of my works (in <strong>chronological order</strong>), representative papers are <span style="background-color: #ffffd0;">highlighted</span>.</p>
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
			    <!-- <td style="padding:20px; width:25%; display: flex; vertical-align:middle;">
      		<div class="one">
                <img src='images/hierarq_teaser.png' width="160" style="height: auto;">
              </div>
            </td> -->
            <td style="padding:20px;width:75%;vertical-align:middle">
                <h3>Streaming Long-form Video Understanding With On-time Answering</h3>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Vibhav Vineet, Yogesh Singh Rawat 
              <br>
              <br>
              <em>Ongoing </em>
              <br>
              <p>
                Developing a novel memory-augmented framework to equip Multimodal Large Language Models (MLLMs) with real-time, continuous understanding of long-form streaming video.
              </p>
            </td>
          </tr>

          <tr id = "disenq" onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <img src='images/disenq_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2507.07262">
                <papertitle>DisenQ: Disentangling Q-Former for Activity-Biometrics</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Yogesh Singh Rawat
              <br><br>
              <em>International Conference on Computer Vision, 2025 <strong>(ICCV)</strong> <strong>Highlight</strong> </em>
              <br>
              <em>Patent pending</em>
              <br>
              <a href="https://sacrcv.github.io/DisenQ-website/">Project Page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/pdf/2507.07262">Paper</a>  
              <p>
                A novel disentanglement-based Multimodal Large Language Model (MLLM) based architecture for robust activity-aware person identification. 
              </p>
            </td>
          </tr>

        <tr id = "geometer" onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <img src='images/geometer_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2408.11748">
                <papertitle>GeoMeter: Understanding Depth and Height Perception in Large Visual Language Models</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Yash Jain, Rishit Garg, Vibhav Vineet, Yogesh Singh Rawat
              <br><br>
              <em>Computer Vision and Pattern Recognition Conference Workshops, 2025 <strong>(CVPR Workshops)</strong> </em>
              <br>
              <em> 3rd Workshop on What is Next in Multimodal Foundation Models</em>
              <br>
              <a href="https://sacrcv.github.io/GeoMeter-website/">Project Page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/pdf/2408.11748">Paper</a>  
              <p></p>
              <p>
                The first diagnostic benchmark for specifically evaluating the depth and height perception capabilities of Multimodal Large Language Models (MLLMs). 
              </p>
            </td>
          </tr>

          <tr id = "hierarq" onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <img src='images/hierarq_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.20678">
                <papertitle>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Vibhav Vineet, Yogesh Singh Rawat
              <br><br>
              <em>Computer Vision and Pattern Recognition Conference, 2025 <strong>(CVPR)</strong></em>
              <br>
              <a href="https://sacrcv.github.io/HierarQ-website/">Project Page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/pdf/2502.20678">Paper</a>  
              <p></p>
              <p>
                A novel task-aware and hierarchical framework to equip Multimodal Large Language Models (MLLMs) for efficient arbitrarity  long-form video understanding. 
              </p>
            </td>
          </tr>

          <tr id = "robustness" onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middl;">
              <div class="one">
                <img src='images/robustness_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.09278">
                <papertitle>Robustness Analysis on Foundation Segmentation Models</papertitle>
              </a>
              <br>
              Madeline Chantry Schiappa, <strong> Shehreen Azad, </strong> Sachidanand VS, Yunhao Ge, Ondrej Miksik, Vibhav Vineet, Yogesh Singh Rawat
              <br><br>
              <em>Computer Vision and Pattern Recognition Conference Workshops, 2024 <strong>(CVPR Workshops)</strong></em>
              <br>
              <em> 2nd Workshop on What is Next in Multimodal Foundation Models</em>
              <br>
              <a href="https://arxiv.org/pdf/2306.09278">Paper</a>  &nbsp/&nbsp
              <a href="https://github.com/DeepLearningRobustnessStudies/SegmetationRobustness.git">Data</a>
              <p></p>
              <p>
                Investigated the robustness of Multimodal Foundation Models (MFMs) in the face of distribution shift caused by perturbations and corruptions across 17 different categories and 5 different severity levels.
              </p>
            </td>
          </tr>

          <tr id = "compositionality" onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <img src='images/compositionality_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.03659">
                <papertitle>Probing Conceptual Understanding of Large Visual Language Models</papertitle>
              </a>
              <br>
              Madeline Chantry Schiappa, Raiyaan Abdullah, <strong> Shehreen Azad, </strong> Jared Claypoole, Michael Cogswell, Ajay Divakaran, Yogesh Singh Rawat
              <br><br>
              <em>Computer Vision and Pattern Recognition Conference Workshops, 2024 <strong>(CVPR Workshops)</strong></em>
              <br>
              <em> 2nd Workshop on What is Next in Multimodal Foundation Models</em>
              <br>
              <a href="https://arxiv.org/pdf/2304.03659">Paper</a>  &nbsp/&nbsp
              <a href="https://github.com/DeepLearningRobustnessStudies/UnderstandingVisualTextModels.git">Data</a>
              <p></p>
              <p>
                Investigated and improved the relational, compositional and contextual understanding of Multimodal Large Language Models (MLLMs) through 3 novel benchmarks.
              </p>
            </td>
          </tr>


          <tr id="abnet" onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <img src='images/abnet_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2403.17360">
                <papertitle>Activity-Biometrics: Person Identification from Daily Activities</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Yogesh Singh Rawat
              <br><br>
              <em>Computer Vision and Pattern Recognition Conference, 2024 <strong>(CVPR)</strong></em>
              <br>
              <em>Divisional patent pending</em>
              <br>
              <!-- <a href="https://sacrcv.github.io/HierarQ-website/">Project Page</a> &nbsp/&nbsp -->
              <a href="https://arxiv.org/pdf/2403.17360">Paper</a>  
              <p></p>
              <p>
                Proposed the novel task of activity-biometrics, which scales traditional gait-based person reID methods to activity-aware person reID. Developed a novel disentanglement based framework to improve robust activity-aware person reID. 
              </p>
            </td>
          </tr>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
                  <tr>
                    <td>
                      <h2>Awards</h2>
                    </td>
                  </tr>
                </tbody></table>
        
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/trophy.png" width="50%" alt="Trophy Image"></td>
                
                  <td width="75%" valign="center">
                  <div>
                      ICCV Travel Grant 2025
                  </div>
                  <br>
                  <div>
                      <strong>3 times</strong> recipient of UCF CS Ranking Incentive Award 
                  </div>
                  <br>
                  <div>
                        <strong>2 times</strong> recipient of UCF Presentation Fellowship Award 
                  </div>
                  <br>
                  <div>
                      <strong>2<sup>nd</sup> place,</strong>
                      IARPA BRIAR: Biometric Recognition and Identification at Altitude and Range
               </div>
               <br>
                  <div>
                      CVPR Travel Grant 2024
                  </div>
                  <br>
                <div>
        <!-- 		        <a href="#"> -->
                        UCF ORCGS Doctoral Fellowship, 2023-2024
        <!-- 		        </a> -->
                </div>
                <br>
            </td>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Professional Service</h2>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              Reviewer, CVPR 2024, 2025, 2026
              <br>
              Reviewer, ICCV 2025
              <br>
              Reviewer, NeurIPS 2024, 2025
              <br>
              Reviewer, BMVC 2025
              <br>
              Reviewer, CVPR MMFM Workshop 2024, 2025
              <br>
			  Reviewer, ICPR 2024
              <br>
            </td>
          </tr>
            
</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This website has been adopted from this <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>





  </body>
</html>
