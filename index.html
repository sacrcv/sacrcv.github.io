<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shehreen Azad</title>

    <meta name="author" content="Shehreen Azad">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shehreen Azad
                </p>
                <p>
                  I am a fourth year PhD student at <a href="https://www.crcv.ucf.edu">Center for Research in Computer Vision (CRCV)</a>, University of Central Florida (UCF), under the supervision of <a href="https://www.crcv.ucf.edu/person/rawat/" style="font-size:15px">Dr Yogesh Rawat</a>. </p>

                  <p style="font-size:15px">
                  I have a broad interest in deep learning and computer vision. My research mainly focuses on <strong>long-form video</strong> understanding and <strong>multimodal reasoning.</strong></p>
                 
                  <p style="color:red;"><b> Currently looking for internship positions for Summer'26! Feel free to drop me an email. </b></p>
                
                </p>
                <p style="text-align:center">
                  <a href="mailto:shehreen.azad@ucf.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=2SqIiLwAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://drive.google.com/file/d/1qvj7svGMu5mtkWekLH5W8qjj15GYi1pN/view?usp=share_link">Resume</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/shehreen-azad-89b1bb26a/">LinkedIn</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile_img.jpg"><img style="width:100%;max-width:100%;" alt="profile photo" src="images/profile_img.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">

		        <h2>Updates</h2>
                  <p>
                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Aug'25:</span> <strong>Broadening Participation Award</strong> to attend ICCV 2025. <br>
                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jun'25:</span> <strong>DisenQ (Fist-author paper)</strong> got accepted in ICCV 2025 as Highlight. ⭐️ <br>

                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'25:</span> <strong> Depth and Height Perception Benchmark (First-author paper)</strong> got accepted in CVPR MMFM Workshop 2025. <br>
                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Feb'25:</span> <strong>HierarQ (First-author paper)</strong> got accepted in CVPR'25. ⭐️ <br>

                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'24:</span> <strong> Distribution Shift Benchmark </strong> got accepted in CVPR MMFM Workshop 2024. <br>
                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'24:</span> <strong> Compositional Reasoning Benchmark </strong> got accepted in CVPR MMFM Workshop 2024. <br>
                    <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Feb'24:</span> <strong>Activity-Biometrics (First-author paper)</strong> got accepted in CVPR'24. ⭐️ <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            
            <p>Below is a selected list of my works (in <strong>chronological order</strong>), representative papers are <span style="background-color: #ffffd0;">highlighted</span>.</p>
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stpro_teaser.png' width="160" height="100">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2412.07072"> -->
                <papertitle>Streaming Long-form Video Understanding With On-time Answering</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Vibhav Vineet, Yogesh Rawat 
              <br>
              <em>Ongoing </em>
              <br>
              <!-- <a href="https://arxiv.org/abs/2412.07072">paper</a> &nbsp/&nbsp -->
              <!-- <a href="https://github.com/AKASH2907/stable-mean-teacher">code</a> -->
              <p></p>
              <p>
                Developing a novel memory-augmented framework for efficient streaming video understanding and proactive answering with Multimodal Large Language Models (MLLMs). 
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stpro_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.20678">
                <papertitle>DisenQ: Disentangling Q-Former for Activity-Biometrics</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Yogesh Rawat
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025 <strong>Highlight</strong>
              <br>
              <em>Patent pending</em>
              <br>
              <a href="https://sacrcv.github.io/DisenQ-website/">Project Page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/abs/2507.07262">Paper</a>  &nbsp/&nbsp
              <p>
                Improved activity-aware person identification through a disentanglement based novel multimodal approach. 
              </p>
            </td>
          </tr>



        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stpro_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.20678">
                <papertitle>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong> Vibhav Vineet, Yogesh Rawat
              <br>
              <em>Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2025
              <br>
              <a href="https://aaryangrg.github.io/research/stpro">Project Page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/abs/2507.07262">Paper</a>  &nbsp/&nbsp
              <p></p>
              <p>
                Improved VLMs grounding capabilities via action composition and complex spatio-temporal scene understanding.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/cospal.png' width="160" height="80">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.17053">
                <papertitle>Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong>
              Zsolt Kira, 
              Yogesh Singh Rawat
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
              <br>
              <a href="https://akash2907.github.io/cospal_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/abs/2501.17053">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AKASH2907/copsal-weakly-stvg">code</a> &nbsp/&nbsp
              <a href="data/Thesis_work_presentation.pdf">poster</a> &nbsp/&nbsp
              <a href="https://huggingface.co/akashkumar29/cospal">huggingface</a>
              <!-- &nbsp/&nbsp -->
              <!-- <a href="images/ssl_active_aaai24.jpg">poster</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=hH3XTg-P2Ks&list=PLd3hlSJsX_InvkagapTwHzZhNu8xUz0NF&ab_channel=AkashKumar">video</a> -->
              <p></p>
              <p>
                Developed first vision language models (VLMs) for dense multimodal video detection task without any labels. Devised context aware and self-paced progressive scene learning approach.
              </p>
            </td>
          </tr>
          
          
          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/smt.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.07072">
                <papertitle>Stable Mean Teacher for Semi-Supervised Video Action Detection</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong>
              Sirshapan Mitra, 
              Yogesh Singh Rawat
              <br>
              <em>Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>)</em>, 2025
              <br>
              <a href="https://akash2907.github.io/smt_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/abs/2412.07072">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AKASH2907/stable-mean-teacher">code</a> &nbsp/&nbsp
              <a href="images/smt_poster_aaai25.pdf">poster</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=_FH1gi_5ptg&list=PLd3hlSJsX_IltI_RTF8XDMkI1pM0AWyJB&ab_channel=AkashKumar">video</a> &nbsp/&nbsp
              <a href="https://huggingface.co/akashkumar29/stable-mean-teacher">huggingface</a>
              <p></p>
              <p>
                Learning from mistakes on labelled set and transfer that learning to pseudo labels from unlabeled set to enhance spatio-temporal localization. 
                Class-agnostic spatio-temporal refinement module and temporal coherency constraint for better spatio-temporal localization.
              </p>
            </td>
          </tr>
          
          

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/semi_active.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2312.07169">
                <papertitle>Semi-supervised Active Learning for Video Action Detection</papertitle>
              </a>
              <br>
              Ayush Singh,
              Aayush J Rana,
              <strong> Shehreen Azad, </strong>
              Shruti Vyas,
              Yogesh Singh Rawat
              <br>
              <em>Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>)</em>, 2024
              <br>
              <a href="https://akash2907.github.io/sslal_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/pdf/2312.07169">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AKASH2907/semi-sup-active-learning">code</a> &nbsp/&nbsp
              <a href="images/ssl_active_aaai24.jpg">poster</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=hH3XTg-P2Ks&list=PLd3hlSJsX_InvkagapTwHzZhNu8xUz0NF&ab_channel=AkashKumar">video</a>
              <p></p>
              <p>
                High-pass filtering for enhanced pseudo labels to improvise spatio-temporal localization. Simple sample augmentation strategy for informative sample selection.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ssl_teaser.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.06010">
                <papertitle>Benchmarking self-supervised video representation learning </papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong>
              Ashlesha Kumar, Vibhav Vineet, Yogesh Singh Rawat
              <br>
              <em> Neural Information Processing (<strong>NeurIPS Workshops</strong>)</em>, 2023 <br>
              <em> 4th Workshop on Self-supervised Learning: Theory and Practices </em>
              <br>
              <a href="https://akash2907.github.io/sslvideo_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://arxiv.org/pdf/2306.06010">paper</a> &nbsp/&nbsp
              <a href="images/sslb_neuripsw23.jpg">poster</a>
              <p></p>
              <p>
                First exhaustive study on impact of pre-training in self-supervised learning for videos. Proposed a simple knowledge distillation 
                approach outperforming previous works with 90% less videos.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/e2essl.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kumar_End-to-End_Semi-Supervised_Learning_for_Video_Action_Detection_CVPR_2022_paper.pdf">
                <papertitle>End-to-End Semi-Supervised Learning for Video Action Detection</papertitle>
              </a>
              <br>
              <strong> Shehreen Azad, </strong>
              Yogesh Singh Rawat
              <br>
              <em>Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://akash2907.github.io/e2essl_webpage/">project page</a> &nbsp/&nbsp
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kumar_End-to-End_Semi-Supervised_Learning_for_Video_Action_Detection_CVPR_2022_paper.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AKASH2907/pi-consistency-activity-detection">code</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=kAJV-w6knKA&ab_channel=UCFCRCV">video</a>
              <p></p>
              <p>
                First end-to-end semi-supervised approach for video action detection task. Short-term and long-term smoothness constraints to exploit spatio-temporal coherency.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/cvprw22.png' width="160" height="80">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2204.07892">
                <papertitle>Video Action Detection: Analysing Limitations and Challenges</papertitle>
              </a>
              <br>
              Rajat Modi, Aayush Rana,<strong>  Shehreen Azad, </strong>
              Praveen Tirupattar, Shruti Vyas, Yogesh Singh Rawat, Mubarak Shah
              <br>
              <em>Computer Vision and Pattern Recognition Conference (<strong>CVPR Workshops</strong>)</em>, 2022 <br>
              <em> 1st Workshop on Vision Datasets Understanding</em>
              <br>
              <a href="https://arxiv.org/abs/2204.07892">paper</a>
              <p></p>
              <p>
                Developed new spatio-temporal surveillance based dataset for real-world challenges.
              </p>
            </td>
          </tr>







          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
